{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Inverse Reinforcement Learning Without RL with Gokul Swamy - #643", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone. Welcome to another episode of the TwiML AI podcast. I'm your host, Sam Charrington, and today I'm joined by Gokul Swamy. Gokul is a PhD student at the Robotics Institute at Carnegie Mellon University. Wherever you're listening to today's show, make sure you're subscribed and be sure to like, rate, and review the show. Gokul, welcome to the podcast. Yeah. Thank you so much for having me. Hey, I'm looking forward to digging into our conversation. We'll be talking about your research into the fields of efficient interactive learning and making good decisions without observable boundaries, with a particular emphasis on a few of the papers that you are presenting at this year's ICML conference. Before we dig into that though, I'd love to have you share a little bit about your background and how you came to the field. Yeah, good question. So I just wrapped up my third year at CMU. I spent a bunch of time there working on different things that are about sort of algorithms for this field called imitation learning, which is broadly speaking about how you can try and learn to make good decisions from data. Before that, I spent a few years at Berkeley where I was a master's and undergrad student, and there I was working on methods for human-robot interaction. Even before that, I grew up in San Diego, where I tried to just maximize the amount of time I spent on the beach. Sounds worthwhile. Yeah. So tell us a little bit about your research interests and the focus of your work. Yeah. So I think over the last few years, what I've been really trying to think about is how we can try and learn to make sequence of decisions well from observing data, some sort of expert demonstrated during the same thing. So perhaps the most intuitive example of this is something like self-driving cars. So you put some person in a car, we strap the car up with sensors, we get them to drive all around Pittsburgh, and then we want to learn a program, let's the car do the same thing. I spent a lot of time thinking about what are the right sorts of algorithms for that problem. I think there's two parts of that that I'm most interested in. The first is how we do this efficiently. I mean efficiently in various senses, both in terms of the amount of data you need to use, the amount of compute you need to use, stuff like that. The other thing I'm really interested in is if we're trying to get a car to do the same thing as a person, they don't have the same observation space in some sense. They don't see exactly the same way. They don't have the exact same sensors. And at that point, there's all these sorts of things that show up that affect a relationship you care about that you don't really observe. So let's say you're trying to predict from some variable X to some variable Y, and there's some unobserved variable U that affects both. This is usually called an unobserved confounder. And here I'm trying to predict from some sort of observations, some sort of how much I want to the steering wheel, how much I want to press the gas pedal, stuff like that. For example, your self-driving car might not be able to pick up on the fact that a hand gesture from somebody in the car across from them actually means something. And when you kind of have this sort of partial observability, I think it's a really interesting question of how do you still learn well? So I've been thinking about both in the sort of idealized setting, how do you do it efficiently, and in the sort of more real world setting where you don't perhaps get the same access to the same pieces of information, how do you make decisions well there? Awesome. And how does that tie into the research that you're presenting at the conference this year? Yeah. So I think we have a few papers at the conference, which I'm very thankful for, and they sort of touch on different parts of those topics. So we have one paper at the main conference, which is really focused on the question of how do we do imitation learning efficiently. So that paper, I think, is really cool because it's a paper where the theory is very elegant. It's not a very sort of complicated idea mathematically, but it really, really works in practice. So we're quite excited about that. Is this the inverse RL paper? Yeah, yeah. So I can maybe talk a little bit about that if that would be good. Yeah, let's dig into it. Sure. So just before you do the title of the paper is inverse reinforcement learning without reinforcement learning, which is an intriguing title. Thank you. Yeah. So broadly speaking, right, in science is we have sort of like forward problems and inverse problems, right? The forward problem is usually, okay, I have some kind of objective function and I'm going to optimize it to get something. And the inverse problem is, okay, given behavior of the optimal thing, what was the thing I was trying to optimize in the first place? And in the sort of inverse reinforcement learning setting, right, the forward problem is reinforcement learning. So I give you a word function. I want you to find the optimal policy under this word function. The inverse problem is, okay, I gave you data from the optimal policy and I want you to figure out what was the word function that was being optimized here. So if you think about the driving example, right, I might want to try to extract a function that tells you, okay, how much does this person care about, you know, staying away from the cars that are in front of them? How much do they care about, you know, adhering to the speed limits, things like that? And I think it's a very natural question to ask, like, well, can't I just write down a function that does that? And it's actually really quite hard when you try to do it, right, because I can tell you that, okay, I definitely care about observing the speed limit. I care about staying away from people that are in front of me. But exactly how much I care about those two things relative to each other is very hard to write down. So I think it makes a lot of sense to try and learn from data. Yeah. Yeah. It sounds difficult enough to do manually when you're only thinking about one of those relationships. But when you're talking about highly dimensional set of features, it sounds very, very difficult. For sure. Yeah. And especially if you really want to have good human-like behavior in a variety of settings, right? Most of the people are just paying attention to one thing when they're driving, right? There's all these things are balancing in their head. So you really do need to try and actually learn from data how to do this. And so talk about the motivation behind trying to apply inverse RL without RL. Yeah. So and maybe was that a motivation or was that a result? So I think the motivation here is really how we can make it inverse reinforcement learning more computationally efficient. So there's pros and cons, I think, to the inverse reinforcement learning approach to things. I think the pro is basically that you don't suffer from something called compounding errors. So the simplest approach you could think of trying to do for imitation learning is basically purely offline supervised learning approach. So what I do is I got a set of expert states. I got a set of expert actions. I just regress between them and I just roll out my car. The issue is that, of course, at some point, you're going to make a mistake and you're going to end up in a situation in which you didn't have any data before. And then you're not going to know what to do and you're just going to keep making mistakes over and over again. So if you kind of look at the early self-driving work in the late 80s, this is sort of what they were trying to do. And, you know, the cars drove a little bit, but as soon as you try to do something hard with them, they didn't work. The benefit of the inverse reinforcement learning approaches is because you're actually, you know, rolling out the learner's policy, seeing how things go, you're able to see, you know, okay, when I turn left, this is actually where I ended up. You're not just looking at states from the expert state distribution. So if you kind of think of it as this sort of set of sort of maybe in the kind of distribution shift setting, basically what interaction gives us is it lets us kind of get samples on the test distribution because we can actually drive and see where we end up. So this is really nice because it lets you, you know, kind of see where you're going to end up. So you're not going to make mistakes you don't expect. But the challenge of course then is that you have to repeatedly interact with the simulator over and over again. And you have to solve these hard reinforcement learning problems. So conceptually, the way inverse reinforcement learning works is basically it's almost like again, but in the space of trajectories. So your generator here is like a policy that's kind of coupled with world models or dynamics kind of give you trajectories. And your discriminator here basically is saying, okay, I want to look at the difference between expert and learner trajectories. And then you do a policy update that is okay, let me take this learned discriminator and use as a reward function and do reinforcement learning with that. So the issue then is that you need to repeatedly solve a hard reinforcement learning problem at every single step of this procedure. So that means you're going to spend a huge number of samples, right? We already know that reinforcement learning is really, really hard to actually do in the real world on problems, right? And if I'm asking you to do it over and over again, well, it's kind of difficult. So our question in this work was, okay, there's all these benefits, the inverse reinforcement learning approach, but there are these severe computational issues. So how can we try and actually speed this up quite a bit? And you arrived at an approach that does not use reinforcement learning in fact? Yeah. So it's effectively an approach that allows us to like learning to make sequential, a sequence of decisions, but without the sort of part that makes reinforcement learning hard, which is exploration, right? So maybe sort of a concrete example I like here is something like, imagine what we want to do is sort of act optimally in some sort of problem that looks basically like going down the paths of a binary tree. And basically I tell you that, okay, the reward function this person cared about is zero everywhere, except for one of the leaf nodes in this tree. And my discriminator says, okay, I'm going to pick this one leaf node to be one, everywhere else to be zero. Then my learner needs to explore the entire tree to figure out where the one node is that is non-zero. And that's a huge waste of time and compute and everything, right? The sort of insight we had in this work was that, well, if I know the states the person actually was in, right? If I knew the person always went to the left in this tree, went to the left most node, I don't actually need to look at the rest of the tree, right? Because they never went there. That's probably the wrong thing to do. So I should be focusing my optimization just on that left most path. And if you kind of think about it that way, I think it's pretty reasonable to say that, okay, if I really just focus on optimizing on states that were related to what I saw the expert do, I should be able to cut out a lot of this sort of unnecessary exploration. And so did you cut out all exploration or did you cut out unnecessary exploration, as you said, by constraining the search base of the states that you're looking at? Yeah, that's a really good question. So if I cut out all exploration, that's very close to doing something that's like fully offline, right? And at that point, we wouldn't really have any robustness to compounding errors, right? Because we could just sort of end up in a place we didn't expect. But if what I do is I have enough exploration that I learned to recover my own mistakes, but not so much that I explore the entire space of the world to figure out do this one thing, I can kind of balance these two things. So I can be both computationally efficient and not sort of end up in situations I don't expect and don't know how to recover from. Sure. You kind of constrain the search base and then you use some alternate method that's not reinforcement learning to kind of navigate through it. Yeah. So what we did was actually, I think, a very simple idea, which was that we basically did something very close to reinforcement learning, but we just changed the start state distribution to be that of the policy you want to imitate. And because of that, right, I'm still doing sequential decision making. I'm still learning to recover from all these step mistakes. But I'm saying, hey, you don't need to start at the top of this tree and figure out how to get to the bottom of the tree every single time. I'm telling you, okay, you're only going to be on the left part of the tree. Just make sure you're good there and then you're fine. So this is kind of how you can sort of constrain the search space, but still actually doing search reinforcement learning. So I think it's really like still, it's sort of, I think, the best of both worlds in the sense that you are getting rid of the part of reinforcement learning that's challenging, which is expiration, while keeping the part of it that is helpful, which is actually learning to recover from your own mistakes. So talk a little bit about how you kind of assessed and evaluated your results for the paper. Yeah. So we thus far only got to try out things in simulation. I'm really excited about trying this out on more real world problems soon. But what we did was we basically picked some of these sort of Majoco sort of OpenAI gym environments. And what we tried to do is basically see, you know, how well can we sort of imitate some expert policy. So we know we trained some policy via reinforcement learning and we said, okay, we want to learn another policy that does the same thing. How can we do this quickly? And what we did was we picked problems that are very challenging expiration problems. So imagine something that you're controlling, like a four legged creature that walks through like a large maze, right? You don't go through every single path in this maze if you're doing it with traditional reinforcement learning. But if we basically tell you, hey, these are the set of waypoints through the maze that you only need to care about, it's a much easier optimization problem. And when we tried it out on like this sort of problem, we saw like rather remarkable improvements in terms of the amount of interaction with the environment needed. And there's the kind of computational benefit of it. But I also would say that I think there's a bit of a safety benefit in the sense that if your environment is actually something that is in the real world, you don't want your learner going around and doing crazy things all the time. You want to sort of keep them in a reasonable place. And in this example, were you worried about kind of extracting waypoints from expert data or did you assume that that was a kind of a downstream or upstream task, depending on perspective? That's a good question. I guess I would say it's an upstream task, but it's one that wasn't too bad. Basically what we could do is basically say that, okay, you know, every time step or every few time steps, you know, where was the expert at that time? And then we could just grab that point and say, okay, your waypoint, we can start the learner from this waypoint and see where they go. And so in terms of, did you have existing data sets that you were able to apply to this and benchmark results or how did you benchmark your performance? Yeah. So what we did is we picked one of these sort of like standard environments that sort of, you know, a bunch of different methods have been tried on. These environments also come with a set of data. So we actually took some of the data sets from this sort of offline RL literature and we used that set of data. And then we sort of implemented each of the methods ourselves. And do you see this? You've used the autonomous vehicle analogy several times in describing this work. Do you see this method scaling up to that application? That's a far way from the four-legged open AI gym type of environment. I mean, if you have four wheels, but I think it's actually a very reasonable application. They're actually, it's one I'm pretty excited about. Another one I'm super excited about is I recently learned that a lot of the routes in Google Maps are now calculated using inverse reinforcement learning. I think it's actually one of the world's most widely deployed machine learning systems now. And I think that these sort of techniques could be really, really useful there for just like drastically reducing the amount of compute you need to do to compute routes and stuff like that. And I think also for self-driving, I think it's the sort of thing where, you know, even to this day, I think a good chunk of the self-driving industry, really they're sort of bread and butter of the way their autonomous systems work is using different sorts of reinforcement learning or using inverse reinforcement learning techniques. So I could see this being applicable to a really wide set of kind of real world systems. And it's also a set of, it's also an application that I think is very reasonable in the sense that one of the sort of tricky parts about our method and one thing we're trying to address in future work is that I need to have a sort of a simulator environment where I can just put the learner in some state to start them off, right? Perhaps for like real world robots is really challenging, right? Like if I'm trying to get the robot to do a back flip, I don't know how I like reset the robot to be flipped halfway up in the air and then start it, right? But a lot of kind of stuff in the self-driving space, training is done in simulation. So this is actually a very reasonable thing. Like it shouldn't be hard at all to just basically just move the car to a different position in the simulator. So it's the sort of thing where I think actually self-driving is a sort of a perfect application of this sort of algorithm. I'm also very excited about the Macbing application because I think both of them are settings where this sort of thing should be able to help a lot. The sort of third application I'm really excited about is kind of in this space of large language models. So if you think about the sort of training of these models, right, there's often a sort of fine tuning stuff at the end called RLHF or reinforcement learning from human feedback. And there, once again, we're using a very expensive reinforcement learning procedure, but we also have data of what we actually wanted, right? We know this was a data that was generated that people preferred. So it feels like we should be able to do something very similar there to basically speed up that search a lot. And there, like, you know, given how compute attempts is to train these models, I could expect this to provide like really strong computational benefits. So I guess what I'm trying to say here is that I think that there is a wide set of problems that kind of satisfy the assumptions required for this method to work out well. And I'm really excited about those applications. But I also think it's a very interesting question to think about for the problems where we can't do this really complicated, you know, agile robotics, what are sort of alternative approaches that could be useful there? Awesome. You've also got a couple of workshop papers at the conference. One is complimenting a policy with a different observation space. Can you tell us a little bit about that one and what you're trying to do there? Yeah, of course. So I think this sort of touches on what I was talking a little bit about earlier, which is, you know, trying to make decisions without, you know, access to all the observed features. So the setup for this paper, I think, was pretty interesting. So let's say we have data of some doctor or set of doctors interacting with some patients. So you know, we get data of what treatment they gave and, you know, what notes they perhaps took down. And, you know, we also see whether the patient got better or worse, things like that. And then what we want to do is we want to learn some sort of decision support system, you know, some sort of computerized agent that's able to help them. But you know, because this agent is a computer rather than a person, they're not going to be able to see the same set of things. They're not going to get this kind of same set of observations. And the question is, okay, how do I actually figure out how to help here? And let me try to give you a concrete example of why this is actually not an easy thing to do. So let's say I have a doctor who is prescribing chemotherapy to patients. And let's say the sort of feature they observe is some test that, you know, tells you with a high probability, you know, whether this person has cancer or not. And let's say this doctor is a really good doctor. So they basically only give you chemotherapy if you actually need it. Otherwise they don't. And let's say just for the sake of argument, our computerized agent here doesn't observe the result of this test. Well, what it's going to see is that every single time someone got chemotherapy, they got better. So it's then going to say, okay, the optimal thing for me to do is prescribe everyone chemotherapy because it only makes people get better. And then if you try to say, okay, and you try to trust the system a bit more, unless, you know, the vast majority of people, you know, had an underlying cancer condition, people are going to get worse as a result. So the question in this paper was, okay, how do we still learn how to do this well? We spend a bunch of time using some techniques from the sort of causal inference literature to try and basically kind of correctly estimate the sort of effective and intervention like giving someone a drug and try to use that to really learn these sort of decisions of systems that are able to actually help. And so is this an application of causal modeling approaches? Yeah, yeah, fully. Yeah. Okay. And so talk a little bit about the specific approach you took. Yeah, yeah. So we sort of considered a couple different settings in the paper and each required a sort of, I guess, kind of different approach or different set of assumptions. So I think maybe the easiest setting was basically, okay, at train time, but not at test time, I get to see both sets of observations. So for whatever reason, I also get to see what the doctor saw. But you know, at test time, I'm not going to get to see that. The system doesn't get to get that. You can basically use this technique called the backdoor adjustment that was pioneered by QtApparel back in the day, and that sort of lets you effectively use an important sampling correction to fix incorrect estimates. So basically, you'd be able to sort of figure out that, hey, you know, it's this feature I didn't observe that actually was causing this positive effect, so I shouldn't like kind of over misattribute it to something else. I shouldn't like overestimate the value of the Qt therapy. Of course, this is somewhat unrealistic setting. So then we spend some time thinking about harder settings. So one setting is, okay, you don't get to see what the doctor was looking at. But I do tell you, you know, what was their probability of taking this action, you know, so how likely was they actually to give this person chemotherapy? And then what you can do is you can again do a sort of important sampling technique to kind of fix things. The sort of hardest setting is when I don't give you either of those. I just give you the actions, and I give you a different set of observations. And if you think about this, this is a really hard problem, right? There's no reason to believe without assumptions you could actually solve this problem, right? Like it's like accessing your self-driving car to be able to stop even though it doesn't see the stop sign. It's a really hard problem. So in this case, you're giving the model a set of actions that doctors took to, you know, treatment actions without showing the outcomes. So we're giving them the actions and the outcomes, but not what the doctor saw when they were choosing to give them that treatment. Ah, not the observations. Yeah, yeah, exactly. Exactly. Yeah, yeah, yeah. If we don't give them the outcomes, they're very, yeah, really hard. So you know, it's like the way I like to think about this, it's almost like imagine if you and I were playing a game and it's to predict the outcome of a coin flip and I get to see the outcome of the coin flip. I'm definitely going to win this game, right? So you can't do this without any assumptions. So in particular, there's this technique from the sort of econometrics literature called a proxy correction, which simply put basically says that even if there's something I don't observe that influences a relationship I care about, so long as I actually have proxies for this, that kind of vary in interesting ways, I'm able to use these to basically kind of filter out the effect of this thing I don't observe. So we actually use where it's sort of a sort of a more modern version of those techniques to basically kind of correctly estimate the effect of actually, you know, giving a person a drug. I mean, it sounds like a lot of what you're trying to do here is to correct for kind of sampling imbalance in your data set. Is that a fair assessment of challenge? That's an interesting question. I guess it's, it's a sort of correcting for sampling balance and it's a very interesting sampling balance where it's one where the sampling was done based on something you don't observe and because of that, if you make conclusions based on this data without accounting for that fact, you're going to draw incorrect conclusions. It's definitely a sort of that sort of thing, but it's a very precise sort of sampling balance. Right. So the generator function of the, you know, what's in your observation set and what's not is kind of out of scope until you've got this whole other set of observations that you just don't have access to and you're trying to as efficiently as possible kind of identify ways to modify the actions you would take on your observations based on some knowledge of what the rest of the world is like. Yeah, that's a great way of putting it. Yep. Exactly. And then the other workshop paper is one called learning shared safety constraints from multitask demonstrations. Talk a little bit about the setting there. Sure. Yeah. So I think for like a variety of tasks you would want, you know, some sort of agent to do there's kind of a background set of shared safety constraints you might care about. Right. So, you know, regardless of whether somebody is cleaning the table or, you know, making a sandwich in your kitchen, they shouldn't set the kitchen on fire. That would be nice. Yeah. Yeah. I think I'd be pretty mad if someone did that. And I think the question is, you know, how we can try and learn these sorts of constraints from demonstrations. And if you think about it, this is really similar to what I was talking about earlier where we're trying to learn rewards from demonstrations. And I think what we're trying to do is use a very sort of similar flavor of approach, but here to try and learn these sort of like safety constraints. And the idea here is that you're, it's similar in a sense to what we talked about previously. You've got a whole set of observations. The observations are all focused on kind of what to do and you want to infer what not to do, but there's certainly a lot of things that aren't done in your observation set. That's a really interesting insight. Exactly. So, I think the way we tried to frame this problem is basically this. Let's say I told you what the task the person was trying to do was. You sort of, you know the reward function. And I also give you what they actually did. So any suboptimal action they took then has to be because, oh, there was a safety constraint, right? Like the reason they didn't, if they were trying to get to the, you know, the destination as fast as possible, right? If they didn't run through all the other cars, it must be because they don't want to hit the other cars. So you can use this sort of comparison between the optimal behavior under the reward function and the actual behavior you saw to try to extract what explains this difference. But if you over apply that heuristic, then you also limit your ability to make the process more efficient, learn better ways to do it, things like that. Is that kind of the core limitation that you're fighting against? Yeah, good question. I think the core limitation we are fighting against is, well, this problem is a really kind of ill-posed problem in some sense. In the sense that if for everything I don't see, I could just say there's a constraint that you can't do that. But it's possible that there was just no need to do that, not that it was unsafe to do that. So the kind of key thing we were focusing on this work is saying, okay, how do we fix that problem? And our insight was that, well, what we need is just a lot of multitask data. And you see people doing all these different things in the environment. Make a sandwich, you see them clean the table, you see them wash the dishes, and in none of these things you see them set the kitchen on fire, you can probably safely assume you're not supposed to set the kitchen on fire. So it's sort of aggregating this data from a lot of different tasks, so you don't kind of learn this overly conservative constraint. In a sense to oversimplify, perhaps, that seems like the obvious solution to the problem. If you're observing or if your model's kind of taking these observations and trying to identify what's unsafe and anything it doesn't see, it's going to deem unsafe, we'll just make sure it sees a lot and a lot of different things. If we're talking about the NRL type of setting, we've previously talked about how expensive that can be. Do you also look at the efficiency aspects of it? Is there something about the way that you approach multitask-ness that helps to deal with that? Yeah, that's a really good question. We didn't really explore it much in this particular paper, but you could actually use the algorithm we talked about in the first paper for just basically resetting the states from the expert demonstrations to basically try and solve this problem faster. You could basically take out the reinforcement learning part of this paper and just stick in the algorithm we had earlier, and I think basically everything would go through, both in theory and in practice. That's why I think I'm particularly excited about the techniques we talked about in the first paper, because I think of it as a hammer that can be used for a really wide set of sequential decision-making problems. What datasets did you use for this particular paper? For this particular paper, we actually wanted to make the problem really hard, so we took some of the standard offline RL benchmarks and we just made them much harder. We actually have a result that I'm really excited about in this paper, which is that we have this agent, four-legged ant running on this maze, and we're able to recover all the walls of the maze, the full structure of the maze, without the ant ever interacting with any of the walls, just by looking at paths through the maze. That was really exciting to me because I was like, this is really good news. I've seen people try to approach this problem before, but usually they're not able to solve anything beyond a linear or tabular problem. It was just here where we were able to do something I was like, I could actually imagine using this. I think the reason that's true for this method is it's built on this strong theoretical and algorithmic foundation of techniques on the inverse reinforcement linear literature, which given they work reliably in the real world for things like self-driving cars and mapping, it's not super surprising they also worked well for this problem. I think by building on a rather solid piece of bedrock, I think I'm really happy that the results worked as well as they did. Because applying that first paper that we talked about, lots of different areas, what are you most looking forward to in terms of your research agenda? That's a good question. I think I have a few different things that I'm still trying to figure out. One of them is the key assumption we're making in that first paper is that we have access to what people in the theory literature will call a generative model access to the environment, which basically means that I can just plop the learner in some random state and then see what they do from there. I'm very interested in the question of, well, if we can't do that, which is true for certain problems, how do we still curtail unnecessary exploration? I think that's a very interesting theoretical question. I don't know if I have a super good answer to it yet, but I think basically you could imagine something of the form that any time you go pretty far outside of the state distribution of the expert, we just assume bad things happen. The learner should learn, hey, I should probably not do that. That's one thing I'm very excited about. Another thing I'm pretty excited about these days is these space of large language models. I think they're a really interesting domain because I think the core way you get them started is with the supervised training. I just spent a bunch of time arguing about basically you should care about these better algorithms. You shouldn't just do supervised learning. Well, then LLMs, you just do supervised learning for at least getting them started and it works great. I think the insight there is, well, if you have a huge amount of data, like a whole internet's worth and you have a model with however many bajillion parameters, then perhaps you don't need to care as much about the right algorithms for these problems. But as soon as you start to get to a problem where you don't have as much data, then I think you need to care more about how do you do things in an efficient manner. I think the place you see that for LLMs is in fine-tuning steps, so specifically the RLHF step. They're right because you actually need to get a person to rate which of these completions was better. You can't get an infinite amount of data of this. At that point, you have to be a lot more careful about algorithms. I'm really excited about trying to do that problem more efficiently. The other problem there that I think is really technically interesting is people always describe RLHF as an alignment procedure. The question is, well, alignment with who? It's like with this pool of raters, all of whom might have different preferences and stuff like that. There's a lot of literature in the economics and social choice theory spaces about basically how do we aggregate different preferences in a way that is reasonable? I think there's a very interesting question there about how do we do that for reinforcement learning and human feedback. I spent some time thinking about that. The other problem which I... It's a little bit on the back burner. One of the settings in which I think you have this flavor of problem I really like, which is that you have repeated interaction and you have partial durability, is problems in the recommendation space. If I'm recommending you content, right? The content I recommend you changes your preferences in some way, right? If I tell you, if I introduce you to a new genre of music, well, then I'm probably going to want to listen to more music like that. I don't actually ever observe your preferences, right? All I observe is that you clicked on this thing. You're in the setting where it's very similar to the settings I was thinking about earlier. I think there's a really interesting space of trying to adapt the algorithms I've talked about previously to these problems. There's some preliminary work that's trying to get at this. I think there's some really beautiful work in the Spotify team on treating recommendation as a sequential decision-making problem rather than just a one-step problem and you see really improved benefits. Actually, I think podcast recommendations in Spotify are now done using reinforcement learning. I think that some of the techniques I've been working on might be really useful for dealing with the fact that, hey, we don't actually observe everything that's going on here and we need to be a little bit more careful about the way we make decisions. Nice. Awesome. Thanks so much for joining us to share a bit about your ICML papers and some of your research. Yeah. Thank you so much for having me. Awesome. Thanks, Coco. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twiMLAI.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}, "podcast_summary": "In this podcast episode, Gokul Swamy, a PhD student at Carnegie Mellon University, discusses his research in the fields of efficient interactive learning and decision-making in the absence of observable boundaries. His focus is on imitation learning and the challenges of learning to make good decisions from data. He explains the concept of inverse reinforcement learning (IRL) and its potential applications, such as self-driving cars and route planning algorithms. Gokul also introduces a paper he presented at the ICML conference that proposes an efficient approach for IRL without reinforcement learning. This method focuses on optimizing states related to the expert's actions without unnecessary exploration, resulting in computational and safety benefits. Another paper discussed is about learning shared safety constraints from multitask demonstrations. The goal is to identify safety constraints from a variety of tasks and use them to guide decision-making in systems like decision support agents. Gokul also highlights the importance of efficient algorithms for large language models and the challenges of fine-tuning and aligning reinforcement learning models with human feedback. He concludes by mentioning the potential application of his research in recommendation systems and the need for adapting the algorithms to handle partial observability in sequential decision-making problems.\n", "podcast_guest": {"guest_name": "Gokul Swamy", "guest_organization": "Robotics Institute at Carnegie Mellon University", "guest_title": null}, "podcast_highlights": "In this podcast episode, Gokul Swamy discusses his research on efficient interactive learning and making good decisions without observable boundaries. He focuses on imitation learning and the challenges of efficiently learning from observed data. One approach presented is inverse reinforcement learning without reinforcement learning, which aims to make the learning process computationally efficient. By focusing on optimizing states that are related to what the expert demonstrated, unnecessary exploration can be curtailed. Another topic discussed is learning shared safety constraints from multitask demonstrations. The goal is to learn safety constraints from demonstrations and avoid overly conservative constraints by aggregating data from different tasks. The research also explores the application of these techniques in the fields of self-driving cars, large language models, and recommendation systems. Overall, the research aims to address the efficiency and effectiveness of learning algorithms in various settings."}